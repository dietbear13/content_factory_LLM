# content_generator.py

import json
import os
import logging

from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
from langchain.chains import LLMChain
from tools.collectors.fact_collector import FactCollector, fetch_articles_from_xmlriver


class ContentGenerator:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –≤ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ.
    –í—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã, –∏–∑–±–µ–≥–∞–µ—Ç —à–∞–±–ª–æ–Ω–æ–≤ –∏ –ª–∏—à–Ω–µ–≥–æ.
    """

    def __init__(self, config_path=None):
        if config_path is None:
            script_dir = os.path.dirname(os.path.realpath(__file__))
            config_path = os.path.join(script_dir, "configs", "content_config.json")

        with open(config_path, "r", encoding="utf-8") as f:
            self.config = json.load(f)

        self.model_name = self.config.get("model_name", "gpt-4")
        self.temperature = self.config.get("temperature", 0.3)
        self.top_p = self.config.get("top_p", 1.0)
        self.presence_penalty = self.config.get("presence_penalty", 0.0)
        self.frequency_penalty = self.config.get("frequency_penalty", 0.0)

        self.default_length = self.config.get("default_length", 400)
        self.style = self.config.get("style", "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π")
        self.tone = self.config.get("tone", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π")

        self.criteria = self.config.get("criteria", {})
        self.use_citations = self.config.get("use_citations", False)
        self.citations_style = self.config.get("citations_style", "APA")
        self.use_fact_tool = self.config.get("use_fact_tool", True)

        self.fact_collector = FactCollector(model_name=self.model_name)

        self.llm = ChatOpenAI(
            model_name=self.model_name,
            temperature=self.temperature,
            top_p=self.top_p,
            presence_penalty=self.presence_penalty,
            frequency_penalty=self.frequency_penalty
        )

        self.system_message_template = """
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä —Ñ–∞–Ω–∞—Ç –º–µ—Ç–æ–¥–æ–≤ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –ú–∞–∫—Å–∏–º–∞ –ò–ª—å—è—Ö–æ–≤–∞ ¬´–ü–∏—à–∏, —Å–æ–∫—Ä–∞—â–∞–π¬ª, —Å–æ–∑–¥–∞—é—â–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–°—Ç–∏–ª—å: {style}. –¢–æ–Ω: {tone}.

üìå –ü—Ä–∏–º–µ—Ä —Å—Ç–∏–ª—è:
–í–æ—Ç –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞, –≤ —Å—Ç–∏–ª–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å—Ç–æ–∏—Ç –ø–∏—Å–∞—Ç—å. –ù–µ –∫–æ–ø–∏—Ä—É–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ ‚Äî —Å–ª–µ–¥—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, —Å—Ç–∏–ª–∏—Å—Ç–∏–∫–µ –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º:

{example_text}

üìå –ü–∏—à–∏ —Ç–æ–ª—å–∫–æ –ø–æ —Å—É—â–µ—Å—Ç–≤—É:
- –ë–µ–∑ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π, –æ–±—â–∏—Ö —Ñ—Ä–∞–∑ –∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏.
- –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –∫–ª–∏—à–µ –∏ —à–∞–±–ª–æ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã.
- –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è.

üìå –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
- 3‚Äì4 –∞–±–∑–∞—Ü–∞ –ø–æ 100‚Äì150 —Å–ª–æ–≤.
- –ö–∞–∂–¥—ã–π –∞–±–∑–∞—Ü –¥–æ–ª–∂–µ–Ω —Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –∞—Å–ø–µ–∫—Ç –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞.

üìå –ö—Ä–∏—Ç–µ—Ä–∏–∏:
{criteria_block}

üìå –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–∫—Ç–∞–º–∏:
- –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ —Ñ–∞–∫—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –¥–∞–Ω—ã:
{relevant_facts}
- –ï—Å–ª–∏ {use_citations} = true ‚Äî –≤—Å—Ç–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –≤ —Å—Ç–∏–ª–µ {citations_style}.
- –ï—Å–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –Ω–µ—Ç ‚Äî –ø–∏—à–∏ –ø–æ –æ–±—â–∏–º –ø—Ä–∏–Ω—Ü–∏–ø–∞–º.
"""

        self.human_message_template = """
–ù–∞–ø–∏—à–∏ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (3‚Äì4 –∞–±–∑–∞—Ü–∞) –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É:
"{headline}"

–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—â–µ–π —Ç–µ–º—ã:
"{global_theme}"

–û–±—ä—ë–º: ~{default_length} —Å–ª–æ–≤.
"""

        criteria_lines = []
        if self.criteria.get("use_examples"):
            criteria_lines.append("- –ü—Ä–∏–≤–æ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã.")
        if self.criteria.get("use_numerical_data"):
            criteria_lines.append("- –ò—Å–ø–æ–ª—å–∑—É–π —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.")
        if self.criteria.get("min_paragraphs") or self.criteria.get("max_paragraphs"):
            criteria_lines.append(f"- –û—Ç {self.criteria.get('min_paragraphs', 2)} –¥–æ {self.criteria.get('max_paragraphs', 4)} –∞–±–∑–∞—Ü–µ–≤.")

        self.criteria_block = "\n".join(criteria_lines) if criteria_lines else "- –ù–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤."

        self.chat_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(self.system_message_template),
            HumanMessagePromptTemplate.from_template(self.human_message_template)
        ])

    def run(self, headline: str, global_theme: str, example_text: str = "") -> str:
        logging.info(f"[ContentGenerator] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: ¬´{headline}¬ª (–≤ —Ç–µ–º–µ: {global_theme})")

        facts = []
        if self.use_fact_tool:
            logging.info("[ContentGenerator] –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ XMLriver –∏ LLM...")
            theme_for_search = headline.split(":")[0] if ":" in headline else headline
            articles = fetch_articles_from_xmlriver(theme_for_search, limit=6)
            facts = self.fact_collector.extract_facts(articles, subheading=headline)

        facts_text = "\n".join(f"- {fact}" for fact in facts) if facts else "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤."

        chain_input = {
            "style": self.style,
            "tone": self.tone,
            "use_citations": str(self.use_citations).lower(),
            "citations_style": self.citations_style,
            "headline": headline,
            "global_theme": global_theme,
            "default_length": self.default_length,
            "criteria_block": self.criteria_block,
            "relevant_facts": facts_text,
            "example_text": example_text.strip()
        }

        chain = LLMChain(llm=self.llm, prompt=self.chat_prompt)
        return chain.run(chain_input)

    # --- NEW CODE ---
    def run_with_facts(self, headline: str, global_theme: str, example_text: str, filtered_facts: list[str]) -> str:
        """
        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ run(), –Ω–æ —Ñ–∞–∫—Ç—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –∏–∑–≤–Ω–µ ‚Äî —Ç–µ, —á—Ç–æ –±—ã–ª–∏ —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω—ã FactFilter'–æ–º.
        """
        logging.info(f"[ContentGenerator] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∑–∞—Ä–∞–Ω–µ–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏: '{headline}'")

        facts_text = "\n".join(f"- {fact}" for fact in filtered_facts) if filtered_facts else "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤."

        chain_input = {
            "style": self.style,
            "tone": self.tone,
            "use_citations": str(self.use_citations).lower(),
            "citations_style": self.citations_style,
            "headline": headline,
            "global_theme": global_theme,
            "default_length": self.default_length,
            "criteria_block": self.criteria_block,
            "relevant_facts": facts_text,
            "example_text": example_text.strip()
        }

        chain = LLMChain(llm=self.llm, prompt=self.chat_prompt)
        return chain.run(chain_input)
