import json
import os
import logging
from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
from langchain.chains import LLMChain
from tools.collectors.fact_collector import FactCollector, fetch_articles_from_xmlriver


class ContentGenerator:
    """
    Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÑƒ Ð² Ð·Ð°Ð´Ð°Ð½Ð½Ð¾Ð¹ Ñ‚ÐµÐ¼Ðµ.
    Ð’ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹, Ð¸Ð·Ð±ÐµÐ³Ð°ÐµÑ‚ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð¾Ð² Ð¸ Ð»Ð¸ÑˆÐ½ÐµÐ³Ð¾.
    """

    def __init__(self, config_path=None):
        config_path = config_path or os.path.join(
            os.path.dirname(os.path.realpath(__file__)), "configs", "content_config.json"
        )
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = json.load(f)

        # ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
        self.model_name = self.config.get("model_name", "gpt-4")
        self.temperature = self.config.get("temperature", 0.3)
        self.top_p = self.config.get("top_p", 1.0)
        self.presence_penalty = self.config.get("presence_penalty", 0.0)
        self.frequency_penalty = self.config.get("frequency_penalty", 0.0)

        # Ð¡Ñ‚Ð¸Ð»ÑŒ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸
        self.default_length = self.config.get("default_length", 400)
        self.style = self.config.get("style", "Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹")
        self.tone = self.config.get("tone", "Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹")
        self.criteria = self.config.get("criteria", {})
        self.use_citations = self.config.get("use_citations", False)
        self.citations_style = self.config.get("citations_style", "APA")
        self.use_fact_tool = self.config.get("use_fact_tool", True)

        self.fact_collector = FactCollector(model_name=self.model_name)

        self.llm = ChatOpenAI(
            model_name=self.model_name,
            temperature=self.temperature,
            top_p=self.top_p,
            presence_penalty=self.presence_penalty,
            frequency_penalty=self.frequency_penalty
        )

        self.criteria_block = self._build_criteria_block()
        self.chat_prompt = self._build_prompt()

    def _build_criteria_block(self) -> str:
        lines = []
        if self.criteria.get("use_examples"):
            lines.append("- ÐŸÑ€Ð¸Ð²Ð¾Ð´Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹.")
        if self.criteria.get("use_numerical_data"):
            lines.append("- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ.")
        if self.criteria.get("min_paragraphs") or self.criteria.get("max_paragraphs"):
            lines.append(f"- ÐžÑ‚ {self.criteria.get('min_paragraphs', 2)} Ð´Ð¾ {self.criteria.get('max_paragraphs', 4)} Ð°Ð±Ð·Ð°Ñ†ÐµÐ².")
        return "\n".join(lines) if lines else "- ÐÐµÑ‚ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÐµÐ²."

    def _build_prompt(self):
        system_template = """
Ð¢Ñ‹ â€” Ð¾Ð¿Ñ‹Ñ‚Ð½Ñ‹Ð¹ Ð°Ð²Ñ‚Ð¾Ñ€ Ñ„Ð°Ð½Ð°Ñ‚ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ ÐœÐ°ÐºÑÐ¸Ð¼Ð° Ð˜Ð»ÑŒÑÑ…Ð¾Ð²Ð° Â«ÐŸÐ¸ÑˆÐ¸, ÑÐ¾ÐºÑ€Ð°Ñ‰Ð°Ð¹Â», ÑÐ¾Ð·Ð´Ð°ÑŽÑ‰Ð¸Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.
Ð¡Ñ‚Ð¸Ð»ÑŒ: {style}. Ð¢Ð¾Ð½: {tone}.

ðŸ“Œ ÐŸÑ€Ð¸Ð¼ÐµÑ€ ÑÑ‚Ð¸Ð»Ñ:
Ð’Ð¾Ñ‚ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ñ‚ÐµÐºÑÑ‚Ð°, Ð² ÑÑ‚Ð¸Ð»Ðµ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ ÑÑ‚Ð¾Ð¸Ñ‚ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ. ÐÐµ ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ â€” ÑÐ»ÐµÐ´ÑƒÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ, ÑÑ‚Ð¸Ð»Ð¸ÑÑ‚Ð¸ÐºÐµ Ð¸ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ°Ð¼:

{example_text}

ðŸ“Œ ÐŸÐ¸ÑˆÐ¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ñƒ:
- Ð‘ÐµÐ· Ð²ÑÑ‚ÑƒÐ¿Ð»ÐµÐ½Ð¸Ð¹, Ð¾Ð±Ñ‰Ð¸Ñ… Ñ„Ñ€Ð°Ð· Ð¸ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð¾Ð² Ð¼ÐµÐ¶Ð´Ñƒ Ñ€Ð°Ð·Ð´ÐµÐ»Ð°Ð¼Ð¸.
- ÐÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐºÐ»Ð¸ÑˆÐµ Ð¸ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹.
- ÐÐµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹ÑÑ.

ðŸ“Œ Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°:
- 3â€“4 Ð°Ð±Ð·Ð°Ñ†Ð° Ð¿Ð¾ 100â€“150 ÑÐ»Ð¾Ð².
- ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð°Ð±Ð·Ð°Ñ† Ð´Ð¾Ð»Ð¶ÐµÐ½ Ñ€Ð°ÑÐºÑ€Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð°ÑÐ¿ÐµÐºÑ‚ Ð¿Ð¾Ð´Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°.

ðŸ“Œ ÐšÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¸:
{criteria_block}

ðŸ“Œ Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ñ„Ð°ÐºÑ‚Ð°Ð¼Ð¸:
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÑ‚Ð¸ Ñ„Ð°ÐºÑ‚Ñ‹, ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ Ð´Ð°Ð½Ñ‹:
{relevant_facts}
- Ð•ÑÐ»Ð¸ {use_citations} = true â€” Ð²ÑÑ‚Ð°Ð²Ð»ÑÐ¹ ÑÑÑ‹Ð»ÐºÐ¸ Ð² ÑÑ‚Ð¸Ð»Ðµ {citations_style}.
- Ð•ÑÐ»Ð¸ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð½ÐµÑ‚ â€” Ð¿Ð¸ÑˆÐ¸ Ð¿Ð¾ Ð¾Ð±Ñ‰Ð¸Ð¼ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð°Ð¼.
"""
        human_template = """
ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ñ€Ð°Ð·Ð²Ñ‘Ñ€Ð½ÑƒÑ‚Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ (3â€“4 Ð°Ð±Ð·Ð°Ñ†Ð°) Ð¿Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÑƒ:
"{headline}"

Ð’ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ð¾Ð±Ñ‰ÐµÐ¹ Ñ‚ÐµÐ¼Ñ‹:
"{global_theme}"

ÐžÐ±ÑŠÑ‘Ð¼: ~{default_length} ÑÐ»Ð¾Ð².
"""

        return ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template(human_template)
        ])

    def _prepare_facts(self, headline: str) -> list[str]:
        if not self.use_fact_tool:
            return []

        logging.info("[ContentGenerator] ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· XMLriver Ð¸ LLM...")
        theme_for_search = headline.split(":")[0] if ":" in headline else headline
        articles = fetch_articles_from_xmlriver(theme_for_search, limit=6)
        return self.fact_collector.extract_facts(articles, subheading=headline)

    def _build_chain_input(self, headline: str, global_theme: str, example_text: str, facts: list[str]) -> dict:
        facts_text = "\n".join(f"- {fact}" for fact in facts) if facts else "ÐÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð²."

        return {
            "style": self.style,
            "tone": self.tone,
            "use_citations": str(self.use_citations).lower(),
            "citations_style": self.citations_style,
            "headline": headline,
            "global_theme": global_theme,
            "default_length": self.default_length,
            "criteria_block": self.criteria_block,
            "relevant_facts": facts_text,
            "example_text": example_text.strip()
        }

    def run(self, headline: str, global_theme: str, example_text: str = "") -> str:
        logging.info(f"[ContentGenerator] Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°: Â«{headline}Â» (Ð² Ñ‚ÐµÐ¼Ðµ: {global_theme})")
        facts = self._prepare_facts(headline)
        return self._run_chain(headline, global_theme, example_text, facts)

    def run_with_facts(self, headline: str, global_theme: str, example_text: str, filtered_facts: list[str]) -> str:
        logging.info(f"[ContentGenerator] Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ñ Ð·Ð°Ñ€Ð°Ð½ÐµÐµ Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ñ„Ð°ÐºÑ‚Ð°Ð¼Ð¸: '{headline}'")
        return self._run_chain(headline, global_theme, example_text, filtered_facts)

    def _run_chain(self, headline: str, global_theme: str, example_text: str, facts: list[str]) -> str:
        chain_input = self._build_chain_input(headline, global_theme, example_text, facts)
        chain = LLMChain(llm=self.llm, prompt=self.chat_prompt)
        return chain.run(chain_input)
